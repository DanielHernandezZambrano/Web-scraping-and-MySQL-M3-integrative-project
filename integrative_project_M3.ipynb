{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proyecto Integrador del Módulo 3 de Henry. Carrera: DataScientist "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importación de las librerias a usar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pymysql\n",
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "fecha_hoy = datetime.date.today()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parte del Web Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_base = 'https://cuspide.com/100-mas-vendidos/'    # Paso 1 obtener html\n",
    "pedido = requests.get(url_base)\n",
    "html_obtenido = pedido.text                           # Paso 2. html pasarlo a texto ya que si no, nos devuelve un object de requests\n",
    "\n",
    "soup = BeautifulSoup(html_obtenido, 'html.parser')    # 3. atribuirlo a soup para 'parsear'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "h3 = soup.find_all('h3')   # aislamos la etiqueta h3 donde se encuentran el titulo y la url\n",
    "\n",
    "# obtenemos el titulo en una lista\n",
    "titulo_obtenido =[]       \n",
    "\n",
    "for n in h3:\n",
    "    titulo = n.find('a').text.strip()\n",
    "    titulo_obtenido.append(titulo)\n",
    "\n",
    " # Obtenemos la url en una lista\n",
    "url_obtenida = []           \n",
    "\n",
    "for url in h3:\n",
    "    u = url.find('a').get('href')\n",
    "    url_obtenida.append(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "# Observar cantidad de libros por url\n",
    "print(len(url_obtenida))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listas para guardar los titulos y las url's que den error\n",
    "titulo_error = []\n",
    "url_error = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> El libro: LA PSICOLOGIA DEL DINERO, no se pudo añadir \n",
      "\n",
      "> El libro: DESTROZA ESTE DIARIO ( A TODO COLOR ), no se pudo añadir \n",
      "\n",
      "> El libro: NUESTRA PARTE DE NOCHE, no se pudo añadir \n",
      "\n",
      "> El libro: DEJA DE SER TU, no se pudo añadir \n",
      "\n",
      "> El libro: EL DUELO ( CUANDO EL DOLOR SE HACE CARNE ), no se pudo añadir \n",
      "\n",
      "> El libro: LA ASISTENTA, no se pudo añadir \n",
      "\n",
      "> El libro: TAYLOR : FROM THE VAULT, no se pudo añadir \n",
      "\n",
      "> El libro: LA CASA DE LOS SUICIDIOS, no se pudo añadir \n",
      "\n",
      "> El libro: TOKIO BLUES – BOOKET VERANO 2023-2024, no se pudo añadir \n",
      "\n",
      "> El libro: HABITOS ATOMICOS, no se pudo añadir \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Listas donde se guardan los precios en pesos y dolar oficial\n",
    "precio_pesos = []\n",
    "precio_dolar = []\n",
    "\n",
    "for e in url_obtenida:   # por cada url, hago un request\n",
    "\n",
    "      url_bucle = e\n",
    "      pedido2 = requests.get(url_bucle)\n",
    "      html = pedido2.text\n",
    "      sopa_bucle = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "      t = sopa_bucle.find('h1').text.strip()\n",
    "\n",
    "      try:\n",
    "         # obtenemos el precio en pesos\n",
    "        precio = sopa_bucle.find('p', class_='price product-page-price')\n",
    "        precio_texto = precio.text.strip().replace('&nbsp;', '').replace('$', '').replace('\\\\xa0', '').replace('.', '')\n",
    "        precio2 = float(precio_texto.replace(',', '.'))\n",
    "        precio_pesos.append(precio2)\n",
    "\n",
    "           # Obtenemos el precio en dólares\n",
    "        precio_dolars = sopa_bucle.find('span', style='font-size: 1.3em').text\n",
    "        precio_dolars2 = float(precio_dolars.replace(',', '.'))\n",
    "        precio_dolar.append(precio_dolars2)\n",
    "\n",
    "      except:\n",
    "         print(f'> El libro: {t}, no se pudo añadir \\n')\n",
    "         titulo_error.append(t)\n",
    "         url_error.append(url_bucle)\n",
    "         precio_pesos.append(None)\n",
    "         precio_dolar.append(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número: 1||| titulo: LA PSICOLOGIA DEL DINERO ||| url: https://cuspide.com/producto/la-psicologia-del-dinero/\n",
      "Número: 2||| titulo: DESTROZA ESTE DIARIO ( A TODO COLOR ) ||| url: https://cuspide.com/producto/destroza-este-diario-a-todo-color/\n",
      "Número: 3||| titulo: NUESTRA PARTE DE NOCHE ||| url: https://cuspide.com/producto/nuestra-parte-de-noche/\n",
      "Número: 4||| titulo: DEJA DE SER TU ||| url: https://cuspide.com/producto/deja-de-ser-tu/\n",
      "Número: 5||| titulo: EL DUELO ( CUANDO EL DOLOR SE HACE CARNE ) ||| url: https://cuspide.com/producto/el-duelo-cuando-el-dolor-se-hace-carne/\n",
      "Número: 6||| titulo: LA ASISTENTA ||| url: https://cuspide.com/producto/la-asistenta/\n",
      "Número: 7||| titulo: TAYLOR : FROM THE VAULT ||| url: https://cuspide.com/producto/taylor-from-the-vault/\n",
      "Número: 8||| titulo: LA CASA DE LOS SUICIDIOS ||| url: https://cuspide.com/producto/la-casa-de-los-suicidios/\n",
      "Número: 9||| titulo: TOKIO BLUES – BOOKET VERANO 2023-2024 ||| url: https://cuspide.com/producto/tokio-blues-booket-verano-2023-2024/\n",
      "Número: 10||| titulo: HABITOS ATOMICOS ||| url: https://cuspide.com/producto/habitos-atomicos/\n"
     ]
    }
   ],
   "source": [
    "# Comprobar que se hayan guardado en las listas de error los datos\n",
    "for i in range(len(url_error)):\n",
    "    print(f'Número: {i+1}||| titulo: {titulo_error[i]} ||| url: {url_error[i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtenemos el dolar blue\n",
    "url_blue = 'https://www.clarin.com/economia/divisas-acciones-bonos?gclid=Cj0KCQiA-62tBhDSARIsAO7twbYBCdmDyspJS1SAuZ8OcXFZZKpEUJx-A93vXK2IlRpKwbhC9_-eEAIaAk6IEALw_wcB'\n",
    "pedido3 = requests.get(url_blue)\n",
    "html3 = pedido3.text\n",
    "sopa_blue = BeautifulSoup(html3, 'html.parser')\n",
    "\n",
    "blue = sopa_blue.find_all('div', class_='selling')\n",
    "blues = []\n",
    "\n",
    "for n in blue:\n",
    "    num = n.find('span').text.strip()\n",
    "    blues.append(num)\n",
    "\n",
    "blue_verdadero = float(blues[2].replace('$', '')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtenemos el precio de cada libro con respecto al dolar blue\n",
    "precio_blue = []\n",
    "\n",
    "for p in range(len(precio_pesos)):\n",
    "    if precio_pesos[p] != None:\n",
    "        precio_blue.append(round(precio_pesos[p]/blue_verdadero, 2))\n",
    "    else:\n",
    "        precio_blue.append(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' creamos dos dataframes a partir de dos diccionarios, uno para los libros scrapeados\n",
    " con los datos completos y otra para la auditoría de errores '''\n",
    "\n",
    "# Dict Libros scrapeados\n",
    "libros_scraping = dict( \n",
    "                titulo = titulo_obtenido,\n",
    "                url = url_obtenida,\n",
    "                precio_pesos = precio_pesos,\n",
    "                precio_dolar = precio_dolar,\n",
    "                precio_dolar_blue = precio_blue,\n",
    "                fecha = fecha_hoy)\n",
    "\n",
    "# Dict Auditoria de errores\n",
    "libros_errores = dict(\n",
    "                titulo = titulo_error,\n",
    "                url = url_error,\n",
    "                fecha = fecha_hoy)\n",
    "\n",
    "df_con_dup = pd.DataFrame(libros_scraping)\n",
    "df_errores = pd.DataFrame(libros_errores)\n",
    "df_sin_dup = df_con_dup.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Creamos dos csv a partir de los dataframes de libros(sin duplicados) y \n",
    " errores (lo hago mayor mente para visualizar lo datos en un csv): '''\n",
    "\n",
    "df_sin_dup.to_csv('Cuspide_100.csv', encoding='utf_8', sep=',', index=False)\n",
    "\n",
    "df_errores.to_csv('Cuspide_100_errores.csv', encoding='utf_8', sep=',', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acá empieza la carga de los datos a MySQL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Establezco la conexión\n",
    "conexion = pymysql.connect(\n",
    "   host='localhost',\n",
    "   user='root',\n",
    "   passwd='12345678',\n",
    "   db='cuspide_100'\n",
    ")\n",
    "cursor = conexion.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creo ambas tablas:\n",
    "\n",
    "# Tabla libros:\n",
    "cursor.execute('''CREATE TABLE IF NOT EXISTS libros (\n",
    "                    id_libro INT NOT NULL AUTO_INCREMENT,\n",
    "                    titulo VARCHAR(150) NOT NULL,\n",
    "                    url VARCHAR(255) NOT NULL,\n",
    "                    precio_pesos DECIMAL(20,2) NOT NULL,\n",
    "                    precio_dolar_oficial DECIMAL(20,2) NOT NULL,\n",
    "                    precio_dolar_blue DECIMAL(20,2) NOT NULL,\n",
    "                    fecha DATE NOT NULL,\n",
    "\n",
    "                    PRIMARY KEY(id_libro) \n",
    "\n",
    ")ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_spanish_ci;''')\n",
    "\n",
    "\n",
    "# Tabla auditoria_errores:\n",
    "cursor.execute('''CREATE TABLE IF NOT EXISTS auditoria_errores (\n",
    "                    id_error INT NOT NULL AUTO_INCREMENT,\n",
    "                    titulo VARCHAR(150) NOT NULL,\n",
    "                    url VARCHAR(255) NOT NULL,\n",
    "                    fecha DATE NOT NULL,\n",
    "               \n",
    "                    PRIMARY KEY(id_error) \n",
    "\n",
    ")ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_spanish_ci;''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convierto las columnas de los dataframes en listas, para luego convertirlas en listas de tuplas:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parte tabla libros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos listas que contengas los datos del dataframe df_sin_dup:\n",
    "\n",
    "titulo = [i for i in df_sin_dup['titulo']]\n",
    "url = [i for i in df_sin_dup['url']]\n",
    "precio = [i for i in df_sin_dup['precio_pesos']]\n",
    "precio_usd_oficial = [i for i in df_sin_dup['precio_dolar']]\n",
    "precio_usd_blue = [i for i in df_sin_dup['precio_dolar_blue']]\n",
    "fecha = [i for i in df_sin_dup['fecha']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertimos a lista de tuplas\n",
    "datos_libros = []\n",
    "\n",
    "for i in range(len(titulo)):\n",
    "    datos_libros.append((titulo[i],url[i],precio[i], precio_usd_oficial[i], precio_usd_blue[i], fecha[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cargamos los datos en la tabla libros\n",
    "cursor.executemany('''INSERT INTO libros (titulo, url, precio_pesos, precio_dolar_oficial, precio_dolar_blue, fecha)\n",
    "                   VALUES (%s,%s,%s,%s,%s,%s)''', datos_libros)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parte tabla auditoria_errores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos listas que contengas los datos del dataframe df_errores:\n",
    "\n",
    "titulo_errores = [i for i in df_errores['titulo']]\n",
    "url_errores = [i for i in df_errores['url']]\n",
    "fecha_errores = [i for i in df_errores['fecha']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertimos a lista de tuplas\n",
    "datos_errores = []\n",
    "\n",
    "for i in range(len(titulo_errores)):\n",
    "    datos_errores.append((titulo_errores[i], url_errores[i], fecha_errores[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cargamos los datos en la tabla auditoria_errores:\n",
    "cursor.executemany('''INSERT INTO auditoria_errores (titulo, url, fecha)\n",
    "                   VALUES (%s,%s,%s)''', datos_errores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hacemos el Commit y cerramos la conexión:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "conexion.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "conexion.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
